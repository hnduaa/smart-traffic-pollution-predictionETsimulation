import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential # type: ignore
from tensorflow.keras.layers import Dense, Input # type: ignore
import xgboost as xgb
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
import joblib

# Configuration de la page Streamlit
st.set_page_config(page_title="Analyse et PrÃ©diction", layout="wide")
st.title("ğŸŒ Analyse et PrÃ©diction - Trafic et Pollution de l'Air")

# Menu de navigation dans la barre latÃ©rale
st.sidebar.title("Navigation")
analysis_type = st.sidebar.radio("Choisissez le type d'analyse :", ["ğŸš¦ Analyse du Trafic", "ğŸŒ Analyse de la Pollution de l'Air", "ğŸ¯ PrÃ©dictions"])

# Fonctions pour l'analyse du trafic
def traffic_analysis():
    st.header("ğŸš¦ Analyse du Trafic")
    # Code de l'analyse du trafic
    # Sidebar for user interaction
st.sidebar.header("ğŸ” Data & Settings")

# Load Dataset
data = pd.read_csv('traffic.csv')

# Validate DateTime Column
if 'DateTime' in data.columns:
    data['DateTime'] = pd.to_datetime(data['DateTime'])
    data['Hour'] = data['DateTime'].dt.hour
    data['Day'] = data['DateTime'].dt.day
    data['Month'] = data['DateTime'].dt.month
    data['Weekday'] = data['DateTime'].dt.weekday
else:
    st.sidebar.error("âš ï¸ 'DateTime' column not found. Please check the dataset.")

# Drop unnecessary columns
if 'ID' in data.columns:
    data.drop(columns=['DateTime', 'ID'], inplace=True)

# Check for missing values
if data.isnull().sum().sum() > 0:
    st.sidebar.warning("âš ï¸ There are missing values in the dataset.")
    st.write("Missing Values:", data.isnull().sum())

# **Traffic Density Analysis**
st.subheader("ğŸ“Š Traffic Density Analysis")
fig, ax = plt.subplots(figsize=(10, 5))
hourly_traffic = data.groupby('Hour')['Vehicles'].mean()
hourly_traffic.plot(kind='bar', ax=ax, color='royalblue', alpha=0.7)
ax.set_title('Average Vehicles by Hour')
ax.set_xlabel('Hour')
ax.set_ylabel('Average Vehicles')
st.pyplot(fig)

# **Traffic Clustering using K-Means**
st.subheader("ğŸš— Traffic Clustering")
features = data[['Junction', 'Vehicles']]
scaler = StandardScaler()
scaled_features = scaler.fit_transform(features)
kmeans = KMeans(n_clusters=3, random_state=42)
data['Cluster'] = kmeans.fit_predict(scaled_features)

fig, ax = plt.subplots(figsize=(10, 5))
sns.scatterplot(x='Junction', y='Vehicles', hue='Cluster', data=data, palette='viridis', ax=ax)
ax.set_title('Traffic Clusters')
st.pyplot(fig)

# **Machine Learning Models for Prediction**
st.subheader("ğŸ§  Traffic Prediction Using Machine Learning")

X = data[['Junction', 'Hour', 'Day', 'Month', 'Weekday']]
y = data['Vehicles']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Train Random Forest Model
rf = RandomForestRegressor(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

# Train Linear Regression Model
lin_reg = LinearRegression()
lin_reg.fit(X_train, y_train)
y_pred_lin = lin_reg.predict(X_test)

# Train XGBoost Model
xgb_model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)

# Train KNN Model
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred_knn = knn.predict(X_test)

# **Train ANN Model (Optional)**
st.sidebar.subheader("ğŸ”¬ Train ANN Model")
if st.sidebar.button("Train ANN Model"):
    ann_model = Sequential([
        Dense(64, input_dim=X_train.shape[1], activation='relu'),
        Dense(32, activation='relu'),
        Dense(1)
    ])
    ann_model.compile(optimizer='adam', loss='mse')
    ann_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=1)
    ann_pred = ann_model.predict(X_test)
else:
    ann_pred = np.zeros_like(y_test)

# **Model Performance Comparison Table**
metrics_df = pd.DataFrame({
    "Model": ["Linear Regression", "Random Forest", "XGBoost", "ANN", "KNN"],
    "R2 Score": [
        r2_score(y_test, y_pred_lin),
        r2_score(y_test, y_pred_rf),
        r2_score(y_test, y_pred_xgb),
        r2_score(y_test, ann_pred),
        r2_score(y_test, y_pred_knn)
    ],
    "Mean Squared Error": [
        mean_squared_error(y_test, y_pred_lin),
        mean_squared_error(y_test, y_pred_rf),
        mean_squared_error(y_test, y_pred_xgb),
        mean_squared_error(y_test, ann_pred),
        mean_squared_error(y_test, y_pred_knn)
    ]
})

st.subheader("ğŸ“Š Model Performance Comparison")
st.dataframe(metrics_df)

# **Dynamic Traffic Light Durations**
st.subheader("â³ Dynamic Traffic Light Timing")

def calculate_light_durations(predicted_vehicles, total_predicted_traffic, min_green_time=20, max_green_time=120):
    green_time = max((predicted_vehicles / total_predicted_traffic) * max_green_time, min_green_time)
    yellow_time = 10  # fixed yellow light duration
    red_time = max_green_time - green_time - yellow_time
    red_time = max(red_time, 0)
    return green_time, red_time, yellow_time

total_predicted_traffic = sum(y_pred_rf)

high_traffic_junctions = data.groupby('Junction')['Vehicles'].mean().sort_values(ascending=False)
for junction in high_traffic_junctions.index[:5]:
    predicted_vehicles = data[data['Junction'] == junction]['Vehicles'].mean()
    green, red, yellow = calculate_light_durations(predicted_vehicles, total_predicted_traffic)
    st.write(f"ğŸš¦ Junction {junction} - Vehicles: {predicted_vehicles:.2f}")
    st.write(f"ğŸŸ¢ Green: {green:.2f} sec | ğŸ”´ Red: {red:.2f} sec | ğŸŸ¡ Yellow: {yellow} sec")

# **Conclusion & Insights**
st.subheader("ğŸ“Œ Key Insights & Recommendations")
st.markdown("- **Junction 1 has the highest predicted traffic**, followed by Junctions 3 and 2.")
st.markdown("- **Random Forest performed best** for traffic predictions.")
st.markdown("- **Dynamic traffic light durations** should be optimized to reduce congestion.")
st.markdown("- **Consider using ANN or XGBoost** for more accurate predictions.")

st.success("âœ… Analysis Complete! Use the sidebar to explore more.")


# Fonctions pour l'analyse de la pollution de l'air
def pollution_analysis():
    st.header("ğŸŒ Analyse de la Pollution de l'Air")

    # Chargement des donnÃ©es
    @st.cache_data
    def load_data():
        data = pd.read_csv('pollution_data_with_etat.csv', encoding='latin1', low_memory=False)
        data = data.replace(['NA', 'na', 'Na', 'nA'], np.nan)
        columns = ['so2', 'no2', 'pm2_5']
        data = data[columns]
        for col in columns:
            data[col] = pd.to_numeric(data[col], errors='coerce')
        data = data.dropna()
        return data

    data = load_data()

    # Analyse des donnÃ©es
    st.subheader("ğŸ“Š Analyse des DonnÃ©es")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("Nombre d'Ã©chantillons", f"{len(data):,}")
    with col2:
        st.metric("SOâ‚‚ moyen", f"{data['so2'].mean():.2f} Âµg/mÂ³")
    with col3:
        st.metric("NOâ‚‚ moyen", f"{data['no2'].mean():.2f} Âµg/mÂ³")

    # Visualisations
    st.write("### Visualisations")
    viz_type = st.selectbox(
        'Choisissez une visualisation:',
        ['Distribution des polluants', 'Matrice de corrÃ©lation']
    )

    if viz_type == 'Distribution des polluants':
        fig, axes = plt.subplots(1, 3, figsize=(15, 5))
        for idx, col in enumerate(['so2', 'no2', 'pm2_5']):
            sns.histplot(data=data, x=col, ax=axes[idx], bins=30)
            axes[idx].set_title(f'Distribution de {col}')
        st.pyplot(fig)

    elif viz_type == 'Matrice de corrÃ©lation':
        fig, ax = plt.subplots(figsize=(8, 6))
        sns.heatmap(data.corr(), annot=True, cmap='coolwarm', ax=ax)
        st.pyplot(fig)

    # EntraÃ®nement des modÃ¨les
    st.subheader("ğŸ¤– EntraÃ®nement des ModÃ¨les")
    X = data.drop('pm2_5', axis=1)
    y = data['pm2_5']
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    selected_models = st.multiselect(
        'Choisissez les modÃ¨les Ã  entraÃ®ner:',
        ['Linear Regression', 'Random Forest', 'XGBoost', 'KNN', 'ANN'],
        default=['Linear Regression', 'Random Forest']
    )

    if st.button('EntraÃ®ner les modÃ¨les'):
        results = {}
        for model_name in selected_models:
            if model_name == 'Linear Regression':
                model = LinearRegression()
                model.fit(X_train_scaled, y_train)
            elif model_name == 'Random Forest':
                model = RandomForestRegressor(random_state=42)
                model.fit(X_train_scaled, y_train)
            elif model_name == 'XGBoost':
                model = xgb.XGBRegressor(random_state=42)
                model.fit(X_train_scaled, y_train)
            elif model_name == 'KNN':
                model = KNeighborsRegressor()
                model.fit(X_train_scaled, y_train)
            elif model_name == 'ANN':
                model = Sequential([
                    Input(shape=(X_train_scaled.shape[1],)),
                    Dense(128, activation='relu'),
                    Dense(64, activation='relu'),
                    Dense(32, activation='relu'),
                    Dense(1, activation='linear')
                ])
                model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])
                model.fit(X_train_scaled, y_train, epochs=50, batch_size=32, verbose=0)
                y_pred = model.predict(X_test_scaled).flatten()
            else:
                continue

            y_pred = model.predict(X_test_scaled)

            rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            mae = mean_absolute_error(y_test, y_pred)
            r2 = r2_score(y_test, y_pred)
            results[model_name] = {"RMSE": rmse, "MAE": mae, "RÂ²": r2}

        st.subheader("ğŸ“Š RÃ©sultats des ModÃ¨les")
        results_df = pd.DataFrame(results).T
        st.dataframe(results_df)

def show_predictions(data):
    """Page de prÃ©dictions"""
    st.header('ğŸ¯ PrÃ©dictions')
    
    try:
        # Charger le modÃ¨le et le scaler
        model = joblib.load('best_model.joblib')
        scaler = joblib.load('scaler.joblib')
        
        # Interface de saisie
        st.write("### Entrez les valeurs des polluants")
        col1, col2 = st.columns(2)
        with col1:
            so2_value = st.number_input('SOâ‚‚ (Âµg/mÂ³)', 
                                       min_value=0.0,
                                       max_value=data['so2'].max(),
                                       value=data['so2'].mean())
        with col2:
            no2_value = st.number_input('NOâ‚‚ (Âµg/mÂ³)',
                                       min_value=0.0,
                                       max_value=data['no2'].max(),
                                       value=data['no2'].mean())

        if st.button('PrÃ©dire PM2.5'):
            # PrÃ©parer les donnÃ©es pour la prÃ©diction
            input_data = pd.DataFrame([[so2_value, no2_value]], columns=['so2', 'no2'])
            input_scaled = scaler.transform(input_data)
            
            # Faire la prÃ©diction
            if hasattr(model, 'predict'):
                prediction = model.predict(input_scaled)[0]
            else:  # Pour le modÃ¨le ANN
                prediction = model.predict(input_scaled).flatten()[0]
            
            # Afficher les rÃ©sultats
            st.success(f"PrÃ©diction de PM2.5: {prediction:.2f} Âµg/mÂ³")
            
            # Ajouter un contexte Ã  la prÃ©diction
            avg_pm25 = data['pm2_5'].mean()
            if prediction > avg_pm25:
                st.warning(f"Cette valeur est supÃ©rieure Ã  la moyenne ({avg_pm25:.2f} Âµg/mÂ³)")
            else:
                st.info(f"Cette valeur est infÃ©rieure Ã  la moyenne ({avg_pm25:.2f} Âµg/mÂ³)")
            
            # Afficher un graphique de contexte
            fig, ax = plt.subplots(figsize=(10, 6))
            sns.histplot(data=data, x='pm2_5', bins=30, ax=ax)
            plt.axvline(prediction, color='red', linestyle='--', label='PrÃ©diction')
            plt.axvline(avg_pm25, color='green', linestyle='--', label='Moyenne')
            plt.legend()
            st.pyplot(fig)
            
    except FileNotFoundError:
        st.error("âš ï¸ Aucun modÃ¨le n'a Ã©tÃ© trouvÃ©. Veuillez d'abord entraÃ®ner les modÃ¨les dans l'onglet 'EntraÃ®nement des ModÃ¨les'.")

# SÃ©lection de l'analyse Ã  afficher
if analysis_type == "ğŸš¦ Analyse du Trafic":
    traffic_analysis()
elif analysis_type == "ğŸŒ Analyse de la Pollution de l'Air":
    pollution_analysis()
elif analysis_type == "ğŸ¯ PrÃ©dictions":
    # Chargement des donnÃ©es
    @st.cache_data
    def load_data():
        data = pd.read_csv('pollution_data_with_etat.csv', encoding='latin1', low_memory=False)
        data = data.replace(['NA', 'na', 'Na', 'nA'], np.nan)
        columns = ['so2', 'no2', 'pm2_5']
        data = data[columns]
        for col in columns:
            data[col] = pd.to_numeric(data[col], errors='coerce')
        data = data.dropna()
        return data

    data = load_data()
    show_predictions(data)